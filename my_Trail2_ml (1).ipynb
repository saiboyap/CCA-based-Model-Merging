{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import copy\n",
        "from sklearn.cross_decomposition import CCA\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Custom utilities for dataset loading and model training\n",
        "# Example:\n",
        "# from utils import load_data, preprocess_data\n"
      ],
      "metadata": {
        "id": "nFGyv7zHywKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def direct_averaging(model_A, model_B):\n",
        "    merged_model = copy.deepcopy(model_A)\n",
        "    for layer_idx, (A_layer, B_layer) in enumerate(zip(model_A.layers, model_B.layers)):\n",
        "        A_weights = A_layer.get_weights()\n",
        "        B_weights = B_layer.get_weights()\n",
        "        merged_weights = (np.array(A_weights) + np.array(B_weights)) / 2\n",
        "        merged_model.layers[layer_idx].set_weights(merged_weights)\n",
        "    return merged_model\n"
      ],
      "metadata": {
        "id": "15VVpeQQyxSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_models(models, test_images):\n",
        "    predictions = [model.predict(test_images) for model in models]\n",
        "    avg_predictions = np.mean(predictions, axis=0)\n",
        "    return avg_predictions\n"
      ],
      "metadata": {
        "id": "KwdfZMuYy0mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def permute_models(model_A, model_B):\n",
        "    merged_model = copy.deepcopy(model_A)\n",
        "\n",
        "    for layer_idx, (A_layer, B_layer) in enumerate(zip(model_A.layers, model_B.layers)):\n",
        "        A_weights = A_layer.get_weights()\n",
        "        B_weights = B_layer.get_weights()\n",
        "\n",
        "        # Create a cost matrix based on differences in weights (use L2 norm)\n",
        "        cost_matrix = np.linalg.norm(A_weights[0] - B_weights[0], axis=1)\n",
        "\n",
        "        # Solve the assignment problem using Hungarian Algorithm\n",
        "        row_idx, col_idx = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "        # Align the neurons by reordering B weights based on the optimal assignment\n",
        "        reordered_B_weights = B_weights[0][col_idx]\n",
        "        B_weights[0] = reordered_B_weights\n",
        "\n",
        "        # Average aligned weights\n",
        "        merged_weights = (A_weights + B_weights) / 2\n",
        "        merged_model.layers[layer_idx].set_weights(merged_weights)\n",
        "\n",
        "    return merged_model\n"
      ],
      "metadata": {
        "id": "fknK5ZXiy4Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ot_fusion_models(model_A, model_B):\n",
        "    # Placeholder for OT Fusion implementation, using permute as a simplified proxy\n",
        "    return permute_models(model_A, model_B)\n"
      ],
      "metadata": {
        "id": "vWjSsM-cy7Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matching_weights(model_A, model_B):\n",
        "    merged_model = copy.deepcopy(model_A)\n",
        "\n",
        "    for layer_idx, (A_layer, B_layer) in enumerate(zip(model_A.layers, model_B.layers)):\n",
        "        A_weights = A_layer.get_weights()\n",
        "        B_weights = B_layer.get_weights()\n",
        "\n",
        "        # Compute the weighted average of both models\n",
        "        merged_weights = (A_weights + B_weights) / 2\n",
        "\n",
        "        merged_model.layers[layer_idx].set_weights(merged_weights)\n",
        "\n",
        "    return merged_model\n"
      ],
      "metadata": {
        "id": "heIC2S6_y99X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zipit_merge(model_A, model_B):\n",
        "    # Placeholder for ZipIt! implementation, simplified as direct averaging\n",
        "    return direct_averaging(model_A, model_B)\n"
      ],
      "metadata": {
        "id": "y5t027ngzGZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated CCA Merge Method\n",
        "def compute_cca_transformations(A_layer_outputs, B_layer_outputs):\n",
        "    # Initialize CCA to align neurons\n",
        "    cca = CCA(n_components=min(A_layer_outputs.shape[1], B_layer_outputs.shape[1]))\n",
        "    A_projected, B_projected = cca.fit_transform(A_layer_outputs, B_layer_outputs)\n",
        "\n",
        "    # Extract transformation matrices\n",
        "    PA = cca.x_rotations_\n",
        "    PB = cca.y_rotations_\n",
        "\n",
        "    return PA, PB\n",
        "\n",
        "# Function to transform B model's weights using CCA projections\n",
        "def transform_weights(A_weights, B_weights, PA, PB):\n",
        "    # Transform B weights using the CCA projection matrices\n",
        "    transformed_B_weights = PB @ B_weights @ PA.T\n",
        "    return transformed_B_weights\n",
        "\n",
        "# Function to merge models layer by layer using CCA\n",
        "def cca_merge(model_A, model_B):\n",
        "    merged_model = copy.deepcopy(model_A)\n",
        "\n",
        "    for layer_idx, (A_layer, B_layer) in enumerate(zip(model_A.layers, model_B.layers)):\n",
        "        A_layer_output = A_layer.output\n",
        "        B_layer_output = B_layer.output\n",
        "\n",
        "        # Compute CCA transformation matrices PA and PB\n",
        "        PA, PB = compute_cca_transformations(A_layer_output, B_layer_output)\n",
        "\n",
        "        # Transform and merge the weights\n",
        "        merged_model.layers[layer_idx].set_weights(\n",
        "            transform_weights(A_layer.get_weights(), B_layer.get_weights(), PA, PB)\n",
        "        )\n",
        "\n",
        "    return merged_model\n"
      ],
      "metadata": {
        "id": "huvVFhOYzHhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_all_methods(models, test_images, test_labels):\n",
        "    # Base models average\n",
        "    base_avg_acc = np.mean([model.evaluate(test_images, test_labels, verbose=0)[1] for model in models])\n",
        "    print(f\"Base models avg. accuracy: {base_avg_acc:.4f}\")\n",
        "\n",
        "    # Ensemble method\n",
        "    ensemble_preds = ensemble_models(models, test_images)\n",
        "    ensemble_acc = np.mean(np.argmax(ensemble_preds, axis=1) == test_labels.squeeze())\n",
        "    print(f\"Ensemble accuracy: {ensemble_acc:.4f}\")\n",
        "\n",
        "    # Direct Averaging\n",
        "    merged_model = direct_averaging(models[0], models[1])\n",
        "    direct_avg_acc = merged_model.evaluate(test_images, test_labels, verbose=0)[1]\n",
        "    print(f\"Direct averaging accuracy: {direct_avg_acc:.4f}\")\n",
        "\n",
        "    # Permute Method\n",
        "    permuted_model = permute_models(models[0], models[1])\n",
        "    permute_acc = permuted_model.evaluate(test_images, test_labels, verbose=0)[1]\n",
        "    print(f\"Permute accuracy: {permute_acc:.4f}\")\n",
        "\n",
        "    # OT Fusion\n",
        "    ot_fusion_model = ot_fusion_models(models[0], models[1])\n",
        "    ot_fusion_acc = ot_fusion_model.evaluate(test_images, test_labels, verbose=0)[1]\n",
        "    print(f\"OT Fusion accuracy: {ot_fusion_acc:.4f}\")\n",
        "\n",
        "    # Matching Weights\n",
        "    matched_model = matching_weights(models[0], models[1])\n",
        "    matching_acc = matched_model.evaluate(test_images, test_labels, verbose=0)[1]\n",
        "    print(f\"Matching Weights accuracy: {matching_acc:.4f}\")\n",
        "\n",
        "    # ZipIt Method\n",
        "    zipit_model = zipit_merge(models[0], models[1])\n",
        "    zipit_acc = zipit_model.evaluate(test_images, test_labels, verbose=0)[1]\n",
        "    print(f\"ZipIt! accuracy: {zipit_acc:.4f}\")\n",
        "\n",
        "    # CCA Merge (ours)\n",
        "    cca_merged_model = cca_merge(models[0], models[1])\n",
        "    cca_acc = cca_merged_model.evaluate(test_images, test_labels, verbose=0)[1]\n",
        "    print(f\"CCA Merge (ours) accuracy: {cca_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "equRbpdmzKaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models (ResNet, VGG, etc.)\n",
        "model_A = tf.keras.applications.ResNet50(include_top=True, weights='imagenet')\n",
        "model_B = tf.keras.applications.VGG16(include_top=True, weights='imagenet')\n",
        "\n",
        "# Load other models (for example, DenseNet)\n",
        "model_C = tf.keras.applications.DenseNet121(include_top=True, weights='imagenet')\n",
        "\n",
        "# Assuming models list\n",
        "models = [model_A, model_B, model_C]\n",
        "\n",
        "# Prepare dataset (CIFAR-10 or ImageNet)\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0\n"
      ],
      "metadata": {
        "id": "bQbfT9OYzNQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3360f0d-07cc-426a-df3b-7cbe43152a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m102967424/102967424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m553467096/553467096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m33188688/33188688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_all_methods(models, test_images, test_labels):\n",
        "    # Ensure models are compiled\n",
        "    for model in models:\n",
        "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Base models average\n",
        "    base_avg_acc = np.mean([model.evaluate(test_images, test_labels, verbose=0)[1] for model in models])\n",
        "    print(f\"Base models avg. accuracy: {base_avg_acc:.4f}\")\n",
        "\n",
        "    # Other methods...\n"
      ],
      "metadata": {
        "id": "qHMSGQIrzT0W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}