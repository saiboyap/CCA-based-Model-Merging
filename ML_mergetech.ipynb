{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqcczmzTTSiq",
        "outputId": "81985f9b-0583-4d4b-b588-ebabc606681b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement ot (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for ot\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision numpy matplotlib ot\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_cifar10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_cifar10 = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_cifar100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_cifar100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 128\n",
        "train_loader_c10 = torch.utils.data.DataLoader(train_cifar10, batch_size=batch_size, shuffle=True)\n",
        "test_loader_c10 = torch.utils.data.DataLoader(test_cifar10, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_loader_c100 = torch.utils.data.DataLoader(train_cifar100, batch_size=batch_size, shuffle=True)\n",
        "test_loader_c100 = torch.utils.data.DataLoader(test_cifar100, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Datasets loaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVp4HmwqV8jo",
        "outputId": "8f367ce8-a74c-415c-ee68-9e253f256265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 60.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:02<00:00, 62.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Datasets loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "def get_vgg11_model(num_classes):\n",
        "    model = models.vgg11(pretrained=False)\n",
        "    model.classifier[6] = nn.Linear(4096, num_classes)\n",
        "    return model\n",
        "\n",
        "def get_resnet20_model(num_classes):\n",
        "    model = models.resnet18(pretrained=False)\n",
        "    model.fc = nn.Linear(512, num_classes)\n",
        "    return model\n",
        "\n",
        "print(\"Models defined successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2f9DZH1V_Bs",
        "outputId": "ede2c689-8f34-4210-9e2b-9e3f1da92be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# Step 1: Load and Prepare the Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# CIFAR-10 and CIFAR-100 datasets\n",
        "train_cifar10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_cifar10 = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_cifar100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_cifar100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Reduced datasets for faster training\n",
        "small_train_c10 = Subset(train_cifar10, random.sample(range(len(train_cifar10)), 1000))\n",
        "small_train_c100 = Subset(train_cifar100, random.sample(range(len(train_cifar100)), 1000))\n",
        "\n",
        "batch_size = 128\n",
        "train_loader_c10 = torch.utils.data.DataLoader(small_train_c10, batch_size=batch_size, shuffle=True)\n",
        "train_loader_c100 = torch.utils.data.DataLoader(small_train_c100, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader_c10 = torch.utils.data.DataLoader(test_cifar10, batch_size=batch_size, shuffle=False)\n",
        "test_loader_c100 = torch.utils.data.DataLoader(test_cifar100, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Datasets loaded successfully.\")\n",
        "\n",
        "# Step 2: Define Simplified VGG11 Model\n",
        "class SimpleVGG(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleVGG, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        # Dynamically calculate the input size for the fully connected layer\n",
        "        self.flattened_size = self._get_flattened_size()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.flattened_size, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def _get_flattened_size(self):\n",
        "        with torch.no_grad():\n",
        "            x = torch.zeros(1, 3, 32, 32)\n",
        "            x = self.features(x)\n",
        "            return x.view(1, -1).size(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def get_simple_vgg11_model(num_classes):\n",
        "    return SimpleVGG(num_classes)\n",
        "\n",
        "print(\"Simplified model architecture defined.\")\n",
        "\n",
        "# Step 3: Define Mixed Precision Training Function\n",
        "def train_model_fast(model, train_loader, epochs=1, max_batches=5, lr=0.01):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        batch_count = 0\n",
        "        for images, labels in train_loader:\n",
        "            if batch_count >= max_batches:\n",
        "                break\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            with autocast():  # Corrected: Removed device_type argument\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            batch_count += 1\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "    return model\n",
        "\n",
        "# Step 4: Train Simplified Models\n",
        "print(\"Training models on smaller datasets with simplified architecture...\")\n",
        "vgg11_c10_model1 = train_model_fast(get_simple_vgg11_model(10), train_loader_c10, epochs=1, max_batches=5)\n",
        "vgg11_c10_model2 = train_model_fast(get_simple_vgg11_model(10), train_loader_c10, epochs=1, max_batches=5)\n",
        "\n",
        "vgg11_c100_model1 = train_model_fast(get_simple_vgg11_model(100), train_loader_c100, epochs=1, max_batches=5)\n",
        "vgg11_c100_model2 = train_model_fast(get_simple_vgg11_model(100), train_loader_c100, epochs=1, max_batches=5)\n",
        "\n",
        "print(\"Training completed.\")\n",
        "\n",
        "# Step 5: Evaluate the Models\n",
        "def evaluate_model(model, test_loader):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "print(\"Evaluating models...\")\n",
        "acc_c10_model1 = evaluate_model(vgg11_c10_model1, test_loader_c10)\n",
        "acc_c10_model2 = evaluate_model(vgg11_c10_model2, test_loader_c10)\n",
        "\n",
        "acc_c100_model1 = evaluate_model(vgg11_c100_model1, test_loader_c100)\n",
        "acc_c100_model2 = evaluate_model(vgg11_c100_model2, test_loader_c100)\n",
        "\n",
        "print(f\"Accuracy of CIFAR-10 Model 1: {acc_c10_model1:.2f}%\")\n",
        "print(f\"Accuracy of CIFAR-10 Model 2: {acc_c10_model2:.2f}%\")\n",
        "print(f\"Accuracy of CIFAR-100 Model 1: {acc_c100_model1:.2f}%\")\n",
        "print(f\"Accuracy of CIFAR-100 Model 2: {acc_c100_model2:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUXEvb6aWBra",
        "outputId": "abb7efcc-c4b5-45d7-a69a-9b4f1a303fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Datasets loaded successfully.\n",
            "Simplified model architecture defined.\n",
            "Training models on smaller datasets with simplified architecture...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-2b5552708b44>:84: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-8-2b5552708b44>:94: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Corrected: Removed device_type argument\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Loss: 2.3019\n",
            "Epoch [1/1], Loss: 2.3039\n",
            "Epoch [1/1], Loss: 4.6043\n",
            "Epoch [1/1], Loss: 4.6050\n",
            "Training completed.\n",
            "Evaluating models...\n",
            "Accuracy of CIFAR-10 Model 1: 9.77%\n",
            "Accuracy of CIFAR-10 Model 2: 10.70%\n",
            "Accuracy of CIFAR-100 Model 1: 0.98%\n",
            "Accuracy of CIFAR-100 Model 2: 0.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pot\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAQC6GEQdAws",
        "outputId": "f7bd4cb2-54d1-45fc-e5b5-330cc84c3a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pot\n",
            "  Downloading POT-0.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from pot) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.10/dist-packages (from pot) (1.13.1)\n",
            "Downloading POT-0.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (865 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.6/865.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pot\n",
            "Successfully installed pot-0.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import linear_sum_assignment\n",
        "import numpy as np\n",
        "from sklearn.cross_decomposition import CCA\n",
        "import torch\n",
        "\n",
        "# Direct Averaging\n",
        "def direct_averaging(model_a, model_b):\n",
        "    averaged_model = {k: (model_a.state_dict()[k] + model_b.state_dict()[k]) / 2\n",
        "                      for k in model_a.state_dict()}\n",
        "    model_a.load_state_dict(averaged_model)\n",
        "    return model_a\n",
        "\n",
        "# Permute\n",
        "def permute_weights(model_a, model_b):\n",
        "    \"\"\"\n",
        "    Aligns the weights of model_b to model_a using permutation matching.\n",
        "    \"\"\"\n",
        "    # Extract the first weight tensor from each model\n",
        "    weight_a = list(model_a.parameters())[0].detach().cpu().numpy()\n",
        "    weight_b = list(model_b.parameters())[0].detach().cpu().numpy()\n",
        "\n",
        "    # Ensure the shapes match\n",
        "    if weight_a.shape != weight_b.shape:\n",
        "        raise ValueError(f\"Shape mismatch between model_a and model_b weights: {weight_a.shape} vs {weight_b.shape}\")\n",
        "\n",
        "    # Handle different tensor dimensions\n",
        "    if weight_b.ndim == 4:  # Convolutional layer weights: [out_channels, in_channels, kernel_height, kernel_width]\n",
        "        out_channels, in_channels, kernel_height, kernel_width = weight_a.shape\n",
        "\n",
        "        # Flatten weights along the in_channels axis\n",
        "        reshaped_a = weight_a.transpose(1, 0, 2, 3).reshape(in_channels, -1)\n",
        "        reshaped_b = weight_b.transpose(1, 0, 2, 3).reshape(in_channels, -1)\n",
        "\n",
        "        # Compute correlation for in_channels only\n",
        "        correlation_matrix = np.corrcoef(reshaped_a, reshaped_b, rowvar=True)\n",
        "        correlation_matrix = correlation_matrix[:in_channels, in_channels:]\n",
        "\n",
        "        # Solve the assignment problem\n",
        "        row_ind, col_ind = linear_sum_assignment(-correlation_matrix)\n",
        "\n",
        "        # Apply the permutation to the in_channels of weight_b\n",
        "        permuted_weight_b = weight_b[:, col_ind, :, :]\n",
        "    elif weight_b.ndim == 2:  # Fully connected layer weights: [out_features, in_features]\n",
        "        correlation_matrix = np.corrcoef(weight_a.T, weight_b.T)\n",
        "        row_ind, col_ind = linear_sum_assignment(-correlation_matrix)\n",
        "        permuted_weight_b = weight_b[:, col_ind]\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported weight dimension: {weight_b.ndim}\")\n",
        "\n",
        "    # Convert permuted weights back to PyTorch tensor\n",
        "    permuted_weight_b = torch.tensor(permuted_weight_b, dtype=torch.float32)\n",
        "\n",
        "    # Load the permuted weights into model_b\n",
        "    model_b_state = model_b.state_dict()\n",
        "    first_key = list(model_b_state.keys())[0]  # Assume first parameter key corresponds to the weight tensor\n",
        "    model_b_state[first_key] = permuted_weight_b\n",
        "\n",
        "    # Update model_b's state_dict\n",
        "    model_b.load_state_dict(model_b_state, strict=False)\n",
        "\n",
        "    return model_b\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# OT Fusion\n",
        "def ot_fusion(model_a, model_b):\n",
        "    \"\"\"\n",
        "    Aligns the weights of model_b to model_a using Optimal Transport Fusion.\n",
        "    \"\"\"\n",
        "    # Extract the first weight tensor from each model\n",
        "    weight_a = list(model_a.parameters())[0].detach().cpu().numpy()\n",
        "    weight_b = list(model_b.parameters())[0].detach().cpu().numpy()\n",
        "\n",
        "    # Ensure the shapes match\n",
        "    if weight_a.shape != weight_b.shape:\n",
        "        raise ValueError(f\"Shape mismatch between model_a and model_b weights: {weight_a.shape} vs {weight_b.shape}\")\n",
        "\n",
        "    # Flatten the weights\n",
        "    weight_a_flat = weight_a.flatten()\n",
        "    weight_b_flat = weight_b.flatten()\n",
        "\n",
        "    # Create a cost matrix as the absolute difference between weights\n",
        "    cost_matrix = np.abs(weight_a_flat[:, None] - weight_b_flat[None, :])\n",
        "\n",
        "    # Solve optimal transport using SciPy's linear_sum_assignment\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "    # Create the transport matrix (binary in this simplified case)\n",
        "    transport_matrix = np.zeros_like(cost_matrix)\n",
        "    transport_matrix[row_ind, col_ind] = 1\n",
        "\n",
        "    # Merge weights using the transport matrix\n",
        "    aligned_weights = (transport_matrix @ weight_b_flat) + ((1 - transport_matrix) @ weight_a_flat)\n",
        "\n",
        "    # Reshape aligned weights back to the original shape\n",
        "    aligned_weights = aligned_weights.reshape(weight_a.shape)\n",
        "\n",
        "    # Convert aligned weights to PyTorch tensor\n",
        "    aligned_weights = torch.tensor(aligned_weights, dtype=torch.float32)\n",
        "\n",
        "    # Update model_a's state_dict with the fused weights\n",
        "    model_a_state = model_a.state_dict()\n",
        "    first_key = list(model_a_state.keys())[0]  # Assume first parameter key corresponds to the weight tensor\n",
        "    model_a_state[first_key] = aligned_weights\n",
        "\n",
        "    # Load updated state_dict into model_a\n",
        "    model_a.load_state_dict(model_a_state, strict=False)\n",
        "\n",
        "    return model_a\n",
        "\n",
        "\n",
        "# ZipIt\n",
        "def zipit_merge(model_a, model_b):\n",
        "    \"\"\"\n",
        "    Merges the weights of model_a and model_b using the ZipIt! method.\n",
        "    \"\"\"\n",
        "    # Extract the state dictionaries from both models\n",
        "    state_dict_a = model_a.state_dict()\n",
        "    state_dict_b = model_b.state_dict()\n",
        "\n",
        "    # Initialize a new state_dict for the merged model\n",
        "    merged_state_dict = {}\n",
        "\n",
        "    # Iterate through all keys in the state_dict\n",
        "    for key in state_dict_a:\n",
        "        if key in state_dict_b:\n",
        "            # Perform element-wise maximum for matching keys\n",
        "            merged_state_dict[key] = torch.max(state_dict_a[key], state_dict_b[key])\n",
        "        else:\n",
        "            # If key is missing in model_b, keep model_a's weights\n",
        "            merged_state_dict[key] = state_dict_a[key]\n",
        "\n",
        "    # Load the merged state_dict into model_a\n",
        "    model_a.load_state_dict(merged_state_dict)\n",
        "\n",
        "    return model_a\n",
        "\n",
        "\n",
        "# CCA Merge\n",
        "def cca_merge(model_a, model_b):\n",
        "    weight_a = list(model_a.parameters())[0].detach().cpu().numpy()\n",
        "    weight_b = list(model_b.parameters())[0].detach().cpu().numpy()\n",
        "\n",
        "    cca = CCA(n_components=min(weight_a.shape[1], weight_b.shape[1]))\n",
        "    cca.fit(weight_a, weight_b)\n",
        "    aligned_a, aligned_b = cca.transform(weight_a, weight_b)\n",
        "    aligned_weights = 0.5 * (aligned_a + aligned_b)\n",
        "    model_a.load_state_dict({'weight': aligned_weights})\n",
        "    return model_a\n",
        "\n",
        "print(\"All fusion methods implemented successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6Iy9QoUWElr",
        "outputId": "ace6c5f8-a367-49d9-d3f8-a82dbed7b33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All fusion methods implemented successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test dataset.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "methods = ['Direct Averaging', 'Permute', 'OT Fusion', 'ZipIt', 'CCA Merge']\n",
        "results = []\n",
        "\n",
        "# Apply all fusion methods\n",
        "for method in methods:\n",
        "    if method == 'Direct Averaging':\n",
        "        merged_model = direct_averaging(vgg11_c10_model1, vgg11_c10_model2)\n",
        "    elif method == 'Permute':\n",
        "        merged_model = permute_weights(vgg11_c10_model1, vgg11_c10_model2)\n",
        "    elif method == 'OT Fusion':\n",
        "        merged_model = ot_fusion(vgg11_c10_model1, vgg11_c10_model2)\n",
        "    elif method == 'ZipIt':\n",
        "        merged_model = zipit_merge(vgg11_c10_model1, vgg11_c10_model2)\n",
        "    elif method == 'CCA Merge':\n",
        "        merged_model = cca_merge(vgg11_c10_model1, vgg11_c10_model2)\n",
        "\n",
        "    # Evaluate the merged model\n",
        "    acc = evaluate_model(merged_model, test_loader_c10)\n",
        "    results.append((method, acc))\n",
        "    print(f\"{method} Accuracy: {acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "NsKMPbsQWHG-",
        "outputId": "6b2142ef-dfba-4bc7-cac7-d9dd389418d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Direct Averaging Accuracy: 11.47%\n",
            "Permute Accuracy: 10.70%\n",
            "OT Fusion Accuracy: 12.09%\n",
            "ZipIt Accuracy: 12.11%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with dim 4. CCA expected <= 2.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-0d71b705a82a>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mmerged_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipit_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg11_c10_model1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvgg11_c10_model2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'CCA Merge'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mmerged_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcca_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg11_c10_model1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvgg11_c10_model2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Evaluate the merged model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-2044aaa6a83e>\u001b[0m in \u001b[0;36mcca_merge\u001b[0;34m(model_a, model_b)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mcca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mcca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0maligned_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maligned_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0maligned_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maligned_a\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maligned_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/cross_decomposition/_pls.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, Y)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1056\u001b[0m             )\n\u001b[1;32m   1057\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1059\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 4. CCA expected <= 2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create results table\n",
        "df = pd.DataFrame(results, columns=[\"Method\", \"Accuracy (%)\"])\n",
        "print(df)\n",
        "\n",
        "# Display results in a bar plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(df[\"Method\"], df[\"Accuracy (%)\"])\n",
        "plt.xlabel('Fusion Methods')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Fusion Methods Comparison on CIFAR-10')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j76lEWcdWJ_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X8gEJbR8WM5b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}